{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00964798",
   "metadata": {},
   "source": [
    "### NLP - Natural Language processing\n",
    "Machines do not understand the language that human speak so a medium or some processing is required for a natural language to able understand it.So it is an area of AI which takes raw, written text(in natural human language), and interprets and transforms it into a form that computer can understand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f042ad0",
   "metadata": {},
   "source": [
    "Example - Virtual assistants, Search Engine, Customer Service, Customer sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ce766",
   "metadata": {},
   "source": [
    "##### NLP Tasks-\n",
    "* Language modelling - Tasks of Predicting the what the next word in a sentence, based on the sequence of words appearing in a given language.\n",
    "* Text classification - Task of assigning the text into known set of categories based on content.\n",
    "* Information Extraction - Extracting the relevant information from text.\n",
    "* Information Retieval - Finding the document/data based on user's query.\n",
    "* Conversational Agent - Building a dialogue system that can converse in human language.\n",
    "* Text Summarization - Create short summary of a large document by retaining the important information.\n",
    "* Question Answering - Ability to answer the question which are posed in natural language in an automatic manner.\n",
    "* Machine Translation - Ability to convert from one language to another language.\n",
    "* Topic Modelling - Uncovering important topics from large collection of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087c7ef",
   "metadata": {},
   "source": [
    "#### Language-\n",
    "Language is the principal method of human communication, consisting of words used in a structureed and conventional way and conveyed by speech, writing or gesture.\n",
    "#### Linguistics -\n",
    "The scientific study of language and its structure , including the study of grammer, syntax and phonetics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c053a",
   "metadata": {},
   "source": [
    "#### Building Blocks of Langauge -\n",
    "* Phonemes - speech & sound\n",
    "* Morphemes & Lexemes - Words\n",
    "* Syntax - phrases & sentences\n",
    "* Context - meaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417b1a6",
   "metadata": {},
   "source": [
    "#### NLP Pipeline-\n",
    "The step-by-step processing of text is known as pipeline.The main components of an NLP pipelines are-\n",
    "* Data collection/acquisition - It is a process of gathering, filtering, and cleaning data before the data is put in a data ware house or any other storage solution.\n",
    "* Text Cleaning - Text cleaning is the process of preparing raw text for NLP so that machines can understand human language.We remove all the other non-textual information like metadata, markup, tags etc.\n",
    "* Pre-processing - To bring your text into a form that is predictable and analyzable for your task.\n",
    "* Feature Engineering -  Capture the charateristics of text into a numerical format which can be understood by ML algorithms. Process of using domain knowledge of the data to create feature that make machine learning algorithms work.\n",
    "* Modelling - Involves application of ML Algorithms to learn the pattern in the dataset.\n",
    "* Evaluation -\n",
    "* Deployment - \n",
    "* Monitoring - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c65b6",
   "metadata": {},
   "source": [
    "#### Text preprocessing technique are -\n",
    "* Lowercasing\n",
    "* Tokenization\n",
    "* Sentence segmentation\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "* POS tagging\n",
    "* Named Entity Recognition\n",
    "* Stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de30be",
   "metadata": {},
   "source": [
    "#### Feature engineering techniques - \n",
    "* Word Embeddings\n",
    "* Count Vectorizer\n",
    "* TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645dcb8",
   "metadata": {},
   "source": [
    "### Data Preprocessing -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4265d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d91164",
   "metadata": {},
   "source": [
    "NLTK has built-in support for dozens of corpora and trained models. To use these within NLTK use the NLTK corpus downloader, >>> nltk.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0891cd3",
   "metadata": {},
   "source": [
    "Corpus - a corpus refers to a collection of texts that are provided and packaged along with the NLTK library itself. These corpora are commonly used for linguistic research, text analysis, and natural language processing tasks. NLTK provides a variety of pre-processed corpora that cover different languages, genres, and topics, making it easier for researchers and developers to work with text data for various purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a66307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/webhav/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b022927",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" corpus refers to a collection of texts that are provided and packaged along with the NLTK library itself. These corpora are commonly used for linguistic research, text analysis, and natural language processing tasks\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77766c",
   "metadata": {},
   "source": [
    "#### Tokenization-\n",
    "Tokenization is the process of breaking a text into individual units called tokens. These tokens can be words, sentences, or subword units, depending on the level of granularity required for the specific task at hand. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "802fc380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpus', 'refers', 'to', 'a', 'collection', 'of', 'texts']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tokens[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d850b",
   "metadata": {},
   "source": [
    "#### Bigrams-\n",
    "Bigrams and trigrams are terms used in the context of n-gram models in natural language processing. N-grams are sequences of n tokens (words, characters, subwords) that appear consecutively in a given text. Bigrams and trigrams are specific cases of n-grams:\n",
    "\n",
    "    Bigrams: Bigrams are sequences of two consecutive tokens in a text. In the context of word tokenization, these tokens are typically words. For example, in the sentence \"The cat sat on the mat,\" the bigrams would be: [\"The cat\", \"cat sat\", \"sat on\", \"on the\", \"the mat\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09771267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('corpus', 'refers'),\n",
       " ('refers', 'to'),\n",
       " ('to', 'a'),\n",
       " ('a', 'collection'),\n",
       " ('collection', 'of'),\n",
       " ('of', 'texts'),\n",
       " ('texts', 'that'),\n",
       " ('that', 'are'),\n",
       " ('are', 'provided'),\n",
       " ('provided', 'and'),\n",
       " ('and', 'packaged'),\n",
       " ('packaged', 'along'),\n",
       " ('along', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'NLTK'),\n",
       " ('NLTK', 'library'),\n",
       " ('library', 'itself'),\n",
       " ('itself', '.'),\n",
       " ('.', 'These'),\n",
       " ('These', 'corpora'),\n",
       " ('corpora', 'are'),\n",
       " ('are', 'commonly'),\n",
       " ('commonly', 'used'),\n",
       " ('used', 'for'),\n",
       " ('for', 'linguistic'),\n",
       " ('linguistic', 'research'),\n",
       " ('research', ','),\n",
       " (',', 'text'),\n",
       " ('text', 'analysis'),\n",
       " ('analysis', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'natural'),\n",
       " ('natural', 'language'),\n",
       " ('language', 'processing'),\n",
       " ('processing', 'tasks')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.bigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc17b2",
   "metadata": {},
   "source": [
    "#### Trigrams\n",
    "    Trigrams: Trigrams are sequences of three consecutive tokens in a text. Similarly, in the context of word tokenization, these tokens are typically words. Using the same sentence, \"The cat sat on the mat,\" the trigrams would be: [\"The cat sat\", \"cat sat on\", \"sat on the\", \"on the mat\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecddc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('corpus', 'refers', 'to'),\n",
       " ('refers', 'to', 'a'),\n",
       " ('to', 'a', 'collection'),\n",
       " ('a', 'collection', 'of'),\n",
       " ('collection', 'of', 'texts'),\n",
       " ('of', 'texts', 'that'),\n",
       " ('texts', 'that', 'are'),\n",
       " ('that', 'are', 'provided'),\n",
       " ('are', 'provided', 'and'),\n",
       " ('provided', 'and', 'packaged'),\n",
       " ('and', 'packaged', 'along'),\n",
       " ('packaged', 'along', 'with'),\n",
       " ('along', 'with', 'the'),\n",
       " ('with', 'the', 'NLTK'),\n",
       " ('the', 'NLTK', 'library'),\n",
       " ('NLTK', 'library', 'itself'),\n",
       " ('library', 'itself', '.'),\n",
       " ('itself', '.', 'These'),\n",
       " ('.', 'These', 'corpora'),\n",
       " ('These', 'corpora', 'are'),\n",
       " ('corpora', 'are', 'commonly'),\n",
       " ('are', 'commonly', 'used'),\n",
       " ('commonly', 'used', 'for'),\n",
       " ('used', 'for', 'linguistic'),\n",
       " ('for', 'linguistic', 'research'),\n",
       " ('linguistic', 'research', ','),\n",
       " ('research', ',', 'text'),\n",
       " (',', 'text', 'analysis'),\n",
       " ('text', 'analysis', ','),\n",
       " ('analysis', ',', 'and'),\n",
       " (',', 'and', 'natural'),\n",
       " ('and', 'natural', 'language'),\n",
       " ('natural', 'language', 'processing'),\n",
       " ('language', 'processing', 'tasks')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d35b44",
   "metadata": {},
   "source": [
    "When dealing with bigrams and trigrams, it's important to note that longer n-grams (e.g., 4-grams, 5-grams) might capture more context, but they can also increase the sparsity of data and require more computational resources. N-grams are often used in conjunction with techniques like smoothing to handle cases where certain n-grams have not been observed in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643520f",
   "metadata": {},
   "source": [
    "### POS Tagging - \n",
    "POS tagging, or Part-of-Speech tagging, is a fundamental task in natural language processing (NLP) that involves assigning a grammatical category (or \"part of speech\") to each word in a text, based on its syntactic and morphological characteristics within the sentence. Parts of speech include categories like nouns, verbs, adjectives, adverbs, pronouns, conjunctions, prepositions, and more. POS tagging is crucial for understanding the structure and meaning of a sentence.\n",
    "\n",
    "* She   - PRON\n",
    "* likes - VERB\n",
    "* to    - PART\n",
    "* read  - VERB\n",
    "* books - NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc60cd",
   "metadata": {},
   "source": [
    "    POS tagging can be performed using rule-based approaches, but more commonly, it is achieved using machine learning techniques, such as Hidden Markov Models (HMMs) or Conditional Random Fields (CRFs). These models learn from annotated training data where words are tagged with their correct parts of speech. Once trained, the models can predict the most likely part of speech for each word in a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "229fdc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/webhav/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0776384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('corpus', 'NN')]\n",
      "[('refers', 'NNS')]\n",
      "[('to', 'TO')]\n",
      "[('a', 'DT')]\n",
      "[('collection', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('texts', 'NN')]\n",
      "[('that', 'IN')]\n",
      "[('are', 'VBP')]\n",
      "[('provided', 'VBN')]\n",
      "[('and', 'CC')]\n",
      "[('packaged', 'VBN')]\n",
      "[('along', 'IN')]\n",
      "[('with', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('NLTK', 'NN')]\n",
      "[('library', 'NN')]\n",
      "[('itself', 'PRP')]\n",
      "[('.', '.')]\n",
      "[('These', 'DT')]\n",
      "[('corpora', 'NNS')]\n",
      "[('are', 'VBP')]\n",
      "[('commonly', 'RB')]\n",
      "[('used', 'VBN')]\n",
      "[('for', 'IN')]\n",
      "[('linguistic', 'JJ')]\n",
      "[('research', 'NN')]\n",
      "[(',', ',')]\n",
      "[('text', 'NN')]\n",
      "[('analysis', 'NN')]\n",
      "[(',', ',')]\n",
      "[('and', 'CC')]\n",
      "[('natural', 'JJ')]\n",
      "[('language', 'NN')]\n",
      "[('processing', 'NN')]\n",
      "[('tasks', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5203fda8",
   "metadata": {},
   "source": [
    "### Stop Words-\n",
    "    Stop words are common words that are often filtered out or removed from text during text analysis or natural language processing (NLP) tasks because they are considered to carry less meaningful information compared to content words. These words are typically short and occur frequently in a language, but they don't provide significant context or contribute to the understanding of the core message of the text. Examples of stop words in English include \"the,\" \"and,\" \"is,\" \"in,\" \"of,\" \"it,\" \"to,\" and \"for.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2b49d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/webhav/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4911507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cf6ab93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abab549f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpus',\n",
       " 'refers',\n",
       " 'collection',\n",
       " 'texts',\n",
       " 'provided',\n",
       " 'packaged',\n",
       " 'along',\n",
       " 'NLTK',\n",
       " 'library',\n",
       " '.',\n",
       " 'These',\n",
       " 'corpora',\n",
       " 'commonly',\n",
       " 'used',\n",
       " 'linguistic',\n",
       " 'research',\n",
       " ',',\n",
       " 'text',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'tasks']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = [i for i in tokens if i not in stop_words]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad5e5d",
   "metadata": {},
   "source": [
    "### Stemming:\n",
    "    Stemming is a text normalization technique used in natural language processing (NLP) and information retrieval to reduce words to their base or root form, known as the \"stem.\" The goal of stemming is to group words with similar meanings together, even if they have different forms due to variations in tense, number, or other inflections.\n",
    "\n",
    "For example, consider the words \"running,\" \"runner,\" and \"runs.\" After stemming, all these words would be reduced to the common stem \"run.\" This process allows for better comparison and analysis of words, as well as the reduction of vocabulary size, which can be important for tasks like document indexing and search.\n",
    "\n",
    "Stemming algorithms apply linguistic rules to remove prefixes, suffixes, and other modifications from words. However, stemming does not guarantee that the resulting stem is a valid word. There can be cases where the stem is not an actual word, or where words with different meanings are reduced to the same stem.\n",
    "\n",
    "Some commonly used stemming algorithms include:\n",
    "\n",
    "* Porter Stemming Algorithm: Developed by Martin Porter, this algorithm applies a series of step-by-step transformations to words to derive their stems. It's one of the most well-known and widely used stemming algorithms.\n",
    "* Snowball Stemmer: This is an extension of the Porter algorithm and supports multiple languages.\n",
    "* Lancaster Stemming Algorithm: A more aggressive stemming algorithm compared to Porter, it often produces shorter stems.\n",
    "* Regex Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e207202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd53ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gener\n",
      "gener\n",
      "gener\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "words = [\"generous\", \"generate\", \"generously\",\"generation\"]\n",
    "for i in words:\n",
    "    print(porter.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bb49e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99e68b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n",
      "generat\n",
      "generous\n",
      "generat\n"
     ]
    }
   ],
   "source": [
    "snow = SnowballStemmer(language='english')\n",
    "words = [\"generous\", \"generate\", \"generously\",\"generation\"]\n",
    "for i in words:\n",
    "    print(porter.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78ef3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05f42163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\n",
      "gen\n",
      "gen\n",
      "gen\n"
     ]
    }
   ],
   "source": [
    "lan =  LancasterStemmer()\n",
    "words = [\"generous\", \"generate\", \"generously\",\"generation\"]\n",
    "for i in words:\n",
    "    print(lan.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c89a50",
   "metadata": {},
   "source": [
    "### Lemmatization-\n",
    "Lemmatization is a text normalization technique used in natural language processing (NLP) to reduce words to their base or dictionary form, known as the \"lemma.\" Unlike stemming, which simply removes prefixes and suffixes to generate a root form, lemmatization involves considering the word's context and part of speech to produce a valid word that makes sense linguistically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e927017",
   "metadata": {},
   "source": [
    "    Lemmatization is more accurate than stemming because it takes into account the linguistic structure and meaning of words. This can be particularly important for languages with rich morphology and irregular word forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4b60475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/webhav/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "557b7a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9551408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n",
      "generate\n",
      "generously\n",
      "generation\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "for i in words:\n",
    "    print(lemma.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc1cf1",
   "metadata": {},
   "source": [
    "### NER(Named Entity Extraction) - \n",
    "Named Entity Recognition (NER) is a key task in Natural Language Processing (NLP) that involves the identification and classification of named entities in unstructured text, such as people, organizations, locations, dates, and other relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aea1fc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/webhav/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /home/webhav/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15d8e78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  (PERSON Smith/NNP)\n",
      "  is/VBZ\n",
      "  from/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  and/CC\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Microsoft/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "sentence = \"John Smith is from New York and works at Microsoft.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = pos_tag(tokens)\n",
    "tree = ne_chunk(tags)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296883a9",
   "metadata": {},
   "source": [
    "### Word Sense Disambiguation -\n",
    "Word Sense Disambiguation is a method by which meaning of word is determined from the context in which it is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb807a79",
   "metadata": {},
   "source": [
    "#### In language same word can can mean differently.\n",
    "    bark -> Refers to outer covering of tree.Other meaning refers to the sound made by a dog.\n",
    "    * Cinnamon comes from the bark of the cinnamon tree\n",
    "    * The dog barked at the strangers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f86d650",
   "metadata": {},
   "source": [
    "#### lesk algorithm is one of the algorithm that can be used to find word sense disambiguation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d2cfc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8047595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('current.a.01') occurring in or belonging to the present time\n",
      "Synset('stream.n.02') dominant course (suggestive of running water) of successive events or ideas\n"
     ]
    }
   ],
   "source": [
    "a1 = lesk(word_tokenize(\"what is the current time\"),\"current\")\n",
    "print(a1,a1.definition())\n",
    "a2 = lesk(word_tokenize(\"air current are strong\"),\"current\")\n",
    "print(a2,a2.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e4a5c",
   "metadata": {},
   "source": [
    "## SpaCy\n",
    "    * spaCy is a free, open-source library for advanced Natural Language Processing(NLP) in Python.\n",
    "    * spaCy is designed specifically for production use.\n",
    "    * Helps you build applications that process and \"understand\" large volumes of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e63e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17e73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d586754",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"She wanted rainbow hair. That's what she told the hairdresser. \n",
    "It should be deep rainbow colors, too. She wasn't interested in pastel rainbow hair.\n",
    "She wanted it deep and vibrant so there was no doubt that she had done this on purpose.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ce5103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_context', '_get_array_attrs', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'copy', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_dict', 'from_disk', 'from_docs', 'from_json', 'get_extension', 'get_lca_matrix', 'has_annotation', 'has_extension', 'has_unknown_spaces', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'noun_chunks', 'noun_chunks_iterator', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_ents', 'set_extension', 'similarity', 'spans', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_dict', 'to_disk', 'to_json', 'to_utf8_array', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "print(dir(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562c80a",
   "metadata": {},
   "source": [
    "So when the text is given to the nlp object it is passed through a pipeline of nlp processess The default operations that the SpaCy NLP object applies to given text include:\n",
    "\n",
    "    *Tokenization: SpaCy breaks down the input text into individual words, punctuation marks, and other meaningful tokens. This process is called tokenization, and it forms the basis for most of the subsequent NLP tasks.\n",
    "\n",
    "    *Part-of-Speech Tagging: The NLP object assigns a part-of-speech tag (like noun, verb, adjective, etc.) to each token in the text, indicating its grammatical role in the sentence.\n",
    "\n",
    "    *Dependency Parsing: SpaCy analyzes the syntactic structure of the text and establishes relationships between tokens, such as subject-verb-object relationships. This information is represented as a dependency tree.\n",
    "\n",
    "    *Named Entity Recognition (NER): The NLP object identifies entities like names of people, organizations, locations, dates, and more within the text.\n",
    "\n",
    "    *Lemmatization: The process of reducing words to their base or root form is called lemmatization. SpaCy performs lemmatization to ensure different inflected forms of a word are reduced to a common base form.\n",
    "\n",
    "    *Sentence Segmentation: The input text is divided into individual sentences, allowing for easier analysis at the sentence level.\n",
    "\n",
    "    *Stop Word Removal: Common words that often don't carry significant meaning, like \"the,\" \"is,\" \"and,\" are removed. This can help in reducing noise and improving the signal-to-noise ratio in the analysis.\n",
    "\n",
    "    *Word Vectors and Embeddings: SpaCy's default models often come with pre-trained word vectors, which can be used to represent words as dense numerical vectors. These vectors capture semantic relationships between words.\n",
    "\n",
    "    *Text Classification (if applicable): Depending on the model and task, SpaCy might also include text classification capabilities to categorize text into predefined classes or labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3515985",
   "metadata": {},
   "source": [
    "### Removing the nlp processes that are not required can substantially increase the efficiency and decrease the operating time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce9d76",
   "metadata": {},
   "source": [
    "### Spacy Tokenization-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e8b9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"She wanted rainbow hair. That's what she told the hairdresser. \\nIt should be deep rainbow colors, too. She wasn't interested in pastel rainbow hair.\\nShe wanted it deep and vibrant so there was no doubt that she had done this on purpose.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9281c027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'wanted', 'rainbow', 'hair', '.', 'That', \"'s\", 'what', 'she', 'told']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [i.text for i in doc]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc750f",
   "metadata": {},
   "source": [
    "### Spacy Lemmatization-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24b0ffff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she', 'want', 'rainbow', 'hair', '.', 'that', 'be', 'what', 'she', 'tell']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemme = [i.lemma_ for i in doc]\n",
    "lemme[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68840319",
   "metadata": {},
   "source": [
    "### Spacy POS tagging-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c890d394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'want', 'rainbow', 'hair', '.', 'that', 'be', 'what', 'she', 'tell']\n",
      "['PRON', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'PRON', 'PRON', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "pos = [i.pos_ for i in doc]\n",
    "print(lemme[:10])\n",
    "print(pos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e3e10",
   "metadata": {},
   "source": [
    "### Spacy tag ( The tag_ attribute returns a fine-grained POS tag for a token.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bb09e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'want', 'rainbow', 'hair', '.', 'that', 'be', 'what', 'she', 'tell']\n",
      "['PRP', 'VBD', 'NN', 'NN', '.', 'DT', 'VBZ', 'WP', 'PRP', 'VBD']\n"
     ]
    }
   ],
   "source": [
    "tag = [i.tag_ for i in doc]\n",
    "print(lemme[:10])\n",
    "print(tag[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85589b8a",
   "metadata": {},
   "source": [
    "### Dependency (the dep_ attribute is used to retrieve the syntactic dependency label of a token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "934573a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nsubj', 'ROOT', 'compound', 'dobj', 'punct', 'nsubj', 'ROOT', 'dative', 'nsubj', 'ccomp']\n",
      "['she', 'want', 'rainbow', 'hair', '.', 'that', 'be', 'what', 'she', 'tell']\n"
     ]
    }
   ],
   "source": [
    "dep = [i.dep_ for i in doc]\n",
    "print(dep[:10])\n",
    "print(lemme[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f721207",
   "metadata": {},
   "source": [
    "### Shape:\n",
    "(It provides a simplified representation of the token's format, where uppercase letters are often represented as \"X,\" lowercase letters as \"x,\" digits as \"d,\" punctuation as \"p,\" and whitespace as \"s.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "831b4166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Xxx', 'xxxx', 'xxxx', 'xxxx', '.', 'Xxxx', \"'x\", 'xxxx', 'xxx', 'xxxx']\n",
      "['she', 'want', 'rainbow', 'hair', '.', 'that', 'be', 'what', 'she', 'tell']\n"
     ]
    }
   ],
   "source": [
    "shape = [i.shape_ for i in doc]\n",
    "print(shape[:10])\n",
    "print(lemme[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8911972",
   "metadata": {},
   "source": [
    "### alpha\n",
    "is_alpha attribute is True for tokens that consist entirely of letters and False for tokens that include non-alphabetic characters like punctuation marks, digits, or symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8920759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, False, True, False, True, True, True]\n",
      "['she', 'want', 'rainbow', 'hair', '.', 'that', 'be', 'what', 'she', 'tell']\n"
     ]
    }
   ],
   "source": [
    "alpha = [i.is_alpha for i in doc]\n",
    "print(alpha[:10])\n",
    "print(lemme[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a686081",
   "metadata": {},
   "source": [
    "### is_stop \n",
    "(the is_stop attribute is a boolean attribute associated with each token that indicates whether the token is a stop word or not. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3576576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, False, False, False, True, True, True, True, False]\n",
      "['she', 'want', 'rainbow', 'hair', '.', 'that', 'be', 'what', 'she', 'tell']\n"
     ]
    }
   ],
   "source": [
    "is_stop = [i.is_stop for i in doc]\n",
    "print(is_stop[:10])\n",
    "print(lemme[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d9db7",
   "metadata": {},
   "source": [
    "### explain :To find the details about particular term use explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f81c0cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"nsubj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac02257",
   "metadata": {},
   "source": [
    "### Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "726c4e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE India\n"
     ]
    }
   ],
   "source": [
    "ner_text = \"I love my country India\"\n",
    "ner_nlp=nlp(ner_text)\n",
    "for i in ner_nlp.ents:\n",
    "    print(i.label_, i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f52a44db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899995cb",
   "metadata": {},
   "source": [
    "### Display NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fcb11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e10949d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I love my country \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(docs=ner_nlp,style=\"ent\",jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5005e654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"1a98ebf8d2ed4d438c5c462071ec5d0f-0\" class=\"displacy\" width=\"925\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">love</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">my</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">country</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">India</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a98ebf8d2ed4d438c5c462071ec5d0f-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a98ebf8d2ed4d438c5c462071ec5d0f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a98ebf8d2ed4d438c5c462071ec5d0f-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a98ebf8d2ed4d438c5c462071ec5d0f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a98ebf8d2ed4d438c5c462071ec5d0f-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a98ebf8d2ed4d438c5c462071ec5d0f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a98ebf8d2ed4d438c5c462071ec5d0f-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a98ebf8d2ed4d438c5c462071ec5d0f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,266.5 L758.0,254.5 742.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(docs=ner_nlp,style=\"dep\",jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acbe8e6",
   "metadata": {},
   "source": [
    "# Text Vectorization/Extraction:\n",
    "Text vectorization in NLP refers to the process of converting textual data, such as words or documents, into numerical vectors. This transformation is essential because many machine learning algorithms and models require numerical input data to perform tasks like classification, clustering, regression, and more. Text vectorization is a fundamental step in representing textual information in a format that can be effectively used by these algorithms.\n",
    "\n",
    "Text vectorization methods aim to capture the semantic meaning and relationships between words or documents while preserving important contextual information. There are several techniques for text vectorization:\n",
    "    \n",
    "    \n",
    "    * Bag-of-Words (BoW): In this approach, a text is represented as a collection of word frequencies, disregarding the order of words. Each unique word becomes a feature, and its frequency in the text becomes the corresponding value in the vector.\n",
    "\n",
    "    * Term Frequency-Inverse Document Frequency (TF-IDF): TF-IDF takes into account not only the frequency of words in a single document but also their importance across the entire corpus. It gives higher weight to words that are frequent in a document but rare across documents.\n",
    "\n",
    "    * Word Embeddings: Word embeddings represent words as dense numerical vectors in a continuous space, capturing semantic relationships between words. Popular word embedding methods include Word2Vec, GloVe, and FastText.\n",
    "\n",
    "    *Doc2Vec: An extension of Word2Vec, Doc2Vec also learns dense vectors for documents, enabling the representation of entire documents in a continuous vector space.\n",
    "\n",
    "    *Word Hashing: In this technique, words are hashed into fixed-length vectors. While it lacks the semantic relationships captured by word embeddings, it can be memory-efficient.\n",
    "\n",
    "    *Subword Embeddings: These embeddings represent words as combinations of subword units, enabling handling of out-of-vocabulary words and capturing morphological information.\n",
    "\n",
    "    *Transformer-based Embeddings: Transformers like BERT, GPT, and their variants have introduced contextualized word embeddings that consider the surrounding context of each word. These embeddings are pre-trained on large corpora and can be fine-tuned for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a2112a",
   "metadata": {},
   "source": [
    "### One hot encoding-\n",
    "One-hot encoding is a basic technique used for text vectorization in natural language processing (NLP). It's a simple way to represent categorical data, such as words or characters, as binary vectors. One-hot encoding is often used for tasks like text classification, where each word in a vocabulary is represented as a unique vector, and the presence or absence of a word is indicated by a binary value.\n",
    "\n",
    "Here's how one-hot encoding works:\n",
    "\n",
    "    Vocabulary Creation: First, you create a vocabulary containing all the unique words in your dataset. Each word is assigned a unique index or position in the vocabulary.\n",
    "\n",
    "    Binary Vectors: For each word in the vocabulary, you create a binary vector of zeros with the same length as the vocabulary size. The position corresponding to the index of the word is set to 1, indicating the presence of that word.\n",
    "\n",
    "Let's illustrate this with a simple example:\n",
    "\n",
    "Suppose you have a vocabulary containing four words: \"cat,\" \"dog,\" \"bird,\" and \"fish.\" The vocabulary and their corresponding indices are:\n",
    "\n",
    "    \"cat\" (0)\n",
    "    \"dog\" (1)\n",
    "    \"bird\" (2)\n",
    "    \"fish\" (3)\n",
    "\n",
    "The one-hot encoded vectors for these words would be:\n",
    "\n",
    "    \"cat\": [1, 0, 0, 0]\n",
    "    \"dog\": [0, 1, 0, 0]\n",
    "    \"bird\": [0, 0, 1, 0]\n",
    "    \"fish\": [0, 0, 0, 1]\n",
    "\n",
    "In practice, one-hot encoding has some limitations and drawbacks:\n",
    "\n",
    "    * High Dimensionality: The resulting vectors can be very high-dimensional, especially for large vocabularies, which can lead to computational inefficiencies and the \"curse of dimensionality.\"\n",
    "\n",
    "    * Lack of Semantic Information: One-hot vectors don't capture semantic relationships between words. All words are treated as orthogonal, making it difficult for models to understand similarities and differences between words.\n",
    "\n",
    "    * Sparsity: The vectors are mostly filled with zeros, making them very sparse and inefficient to store.\n",
    "\n",
    "Because of these limitations, more advanced vectorization techniques like word embeddings (e.g., Word2Vec, GloVe, etc.) and contextual embeddings (e.g., BERT, GPT) have become more popular in modern NLP, as they capture semantic information and address the drawbacks of one-hot encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f5f447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21225e8",
   "metadata": {},
   "source": [
    "#### To use sklearn for one hot encoding first do label encoding and then do one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39baf9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"dog bites meat\"\n",
    "doc2 = \"man eats meat\"\n",
    "doc3 = \"dog bites man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac635076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'bites', 'meat', 'man', 'eats', 'meat', 'dog', 'bites', 'man']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_ = [doc1.split(),doc2.split(),doc3.split()]\n",
    "corpus = corpus_[0] + corpus_[1] + corpus_[2]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e1dda57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 4, 3, 2, 4, 1, 0, 3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "data = le.fit_transform(corpus)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23941bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit_transform(corpus_).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59be6c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.transform([\"dog eats man\".split()]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b36ea",
   "metadata": {},
   "source": [
    "    while you can technically apply one-hot encoding to textual data using sklearn, it's usually more effective to use other techniques, such as word embeddings, for capturing semantic information and relationships between words in NLP tasks. One-hot encoding might still be suitable for simple tasks or situations where you need to convert categorical features into numerical form, but it's less suitable for capturing the complexities of language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd25b5e",
   "metadata": {},
   "source": [
    "## Bag of Words - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4a7b0",
   "metadata": {},
   "source": [
    "The Bag of Words (BoW) model is a fundamental concept in natural language processing (NLP) used for representing text data in a numerical format. It's a simple and widely used technique that focuses on the frequency of words in a document, disregarding their order and structure. The BoW model treats a document as a \"bag\" of words, where each word is considered as an independent feature and its frequency is used to create a vector representation.\n",
    "\n",
    "Here's how the Bag of Words model works:\n",
    "\n",
    "    *Vocabulary Creation: Create a vocabulary containing all unique words in your corpus. Each unique word is assigned a unique index.\n",
    "\n",
    "    *Word Frequency: Count the frequency of each word in each document. This creates a frequency vector for each document, where each element of the vector corresponds to the frequency of a specific word in the document.\n",
    "\n",
    "    *Vector Representation: Represent each document as a numerical vector using the word frequencies. The length of the vector is equal to the size of the vocabulary.\n",
    "\n",
    "    *Sparse Vectors: Since most documents contain only a small subset of the entire vocabulary, the vectors are usually sparse (containing mostly zeros).\n",
    "\n",
    "Here's a simple example to illustrate the BoW model:\n",
    "\n",
    "Consider the following two sentences:\n",
    "\n",
    "    \"I love natural language processing.\"\n",
    "    \"I enjoy learning about machine learning.\"\n",
    "\n",
    "The vocabulary would be: [\"I\", \"love\", \"natural\", \"language\", \"processing\", \"enjoy\", \"learning\", \"about\", \"machine\"]\n",
    "\n",
    "The BoW representation of these sentences would be:\n",
    "\n",
    "    [1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "    [1, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "The BoW model has its strengths and limitations:\n",
    "\n",
    "Strengths:\n",
    "\n",
    "    *Simple and easy to implement.\n",
    "    *Provides a basic representation of the document's content.\n",
    "    *Can be used as input for various machine learning algorithms.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "    *Ignores word order and context, leading to a loss of important semantic information.\n",
    "    *Treats all words as independent, which can result in poor representation for complex text.\n",
    "    *Doesn't account for synonyms or variations of words.\n",
    "    *Can lead to high-dimensional, sparse vectors for large vocabularies.\n",
    "\n",
    "While the Bag of Words model is limited in capturing nuances of language, it serves as a starting point for many NLP applications and can be enhanced with techniques like TF-IDF (Term Frequency-Inverse Document Frequency) weighting or more advanced word embeddings to capture semantics and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9293f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "270eb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9d2b68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Harry Potter is an amazing movie!!\"\n",
    "doc2 = \"Harry Potter is the best movie \"\n",
    "doc3 = \"Harry Potter is so great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a7a63201",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_out = cv.fit_transform([doc1,doc2,doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ba92084b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_out.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906dd79b",
   "metadata": {},
   "source": [
    "### BOW N Grams-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "19e4b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,3))#Bigram, trigram\n",
    "cv_out = cv.fit_transform([doc1, doc2, doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2eeff300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>amazing movie</th>\n",
       "      <th>an</th>\n",
       "      <th>an amazing</th>\n",
       "      <th>an amazing movie</th>\n",
       "      <th>best</th>\n",
       "      <th>best movie</th>\n",
       "      <th>great</th>\n",
       "      <th>harry</th>\n",
       "      <th>harry potter</th>\n",
       "      <th>...</th>\n",
       "      <th>potter</th>\n",
       "      <th>potter is</th>\n",
       "      <th>potter is an</th>\n",
       "      <th>potter is so</th>\n",
       "      <th>potter is the</th>\n",
       "      <th>so</th>\n",
       "      <th>so great</th>\n",
       "      <th>the</th>\n",
       "      <th>the best</th>\n",
       "      <th>the best movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amazing  amazing movie  an  an amazing  an amazing movie  best  best movie  \\\n",
       "0        1              1   1           1                 1     0           0   \n",
       "1        0              0   0           0                 0     1           1   \n",
       "2        0              0   0           0                 0     0           0   \n",
       "\n",
       "   great  harry  harry potter  ...  potter  potter is  potter is an  \\\n",
       "0      0      1             1  ...       1          1             1   \n",
       "1      0      1             1  ...       1          1             0   \n",
       "2      1      1             1  ...       1          1             0   \n",
       "\n",
       "   potter is so  potter is the  so  so great  the  the best  the best movie  \n",
       "0             0              0   0         0    0         0               0  \n",
       "1             0              1   0         0    1         1               1  \n",
       "2             1              0   1         1    0         0               0  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv_out.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5290035e",
   "metadata": {},
   "source": [
    "# TF-IDF(Term frequency-inverse document frequency):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b7e40",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency-Inverse Document Frequency, and it's a numerical representation of words in a document collection that helps to capture their importance or relevance within each document relative to the entire corpus. TF-IDF is a common technique used in natural language processing (NLP) for text vectorization and information retrieval tasks.\n",
    "\n",
    "Here's how TF-IDF works:\n",
    "\n",
    "    *Term Frequency (TF): For each word in a document, calculate the frequency of that word within the document. This represents how often a word appears in a specific document.\n",
    "\n",
    "    TF = (Number of occurrences of a word in the document) / (Total number of words in the document)\n",
    "\n",
    "    *Inverse Document Frequency (IDF): IDF measures how unique or rare a word is across the entire document collection. It's calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the word.\n",
    "\n",
    "    IDF = log((Total number of documents) / (Number of documents containing the word))\n",
    "\n",
    "    TF-IDF Score: The TF-IDF score for a word in a document is calculated by multiplying its TF by its IDF.\n",
    "\n",
    "    TF-IDF = TF * IDF\n",
    "\n",
    "The TF-IDF score reflects how important a word is to a specific document while taking into account its importance across the entire corpus. Words that appear frequently in a document but rarely in other documents receive higher TF-IDF scores, indicating their relevance to that specific document.\n",
    "\n",
    "Key points about TF-IDF:\n",
    "\n",
    "    *High TF-IDF score: A word that is frequent in a specific document but rare in other documents.\n",
    "    *Low TF-IDF score: A word that is either common across all documents or rare in the specific document.\n",
    "\n",
    "Applications of TF-IDF in NLP:\n",
    "\n",
    "    *Information Retrieval: TF-IDF is used in search engines to rank documents based on their relevance to a query.\n",
    "\n",
    "    *Text Classification: It's used as a feature extraction technique for machine learning models in tasks like sentiment analysis and topic classification.\n",
    "\n",
    "    *Document Clustering: TF-IDF vectors can be used for clustering similar documents together.\n",
    "\n",
    "    *Keyword Extraction: TF-IDF can help identify the most important keywords within a document.\n",
    "\n",
    "    *Document Summarization: TF-IDF can be used to select the most important sentences or phrases in a document for summarization.\n",
    "\n",
    "However, TF-IDF has its limitations, such as not capturing semantic meaning or word relationships. In cases where you need to capture semantic information, word embeddings or contextual embeddings (such as BERT) might be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "10de6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6c959961",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Data science is an amazing career in the current world\"\n",
    "doc2 = \"Deep learning is a sub set of machine learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f2070b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()\n",
    "result = vec.fit_transform([doc1, doc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "058ad140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32433627, 0.32433627, 0.32433627, 0.32433627, 0.32433627,\n",
       "        0.        , 0.32433627, 0.23076793, 0.        , 0.        ,\n",
       "        0.        , 0.32433627, 0.        , 0.        , 0.32433627,\n",
       "        0.32433627],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.32433627, 0.        , 0.23076793, 0.64867255, 0.32433627,\n",
       "        0.32433627, 0.        , 0.32433627, 0.32433627, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cdcbd40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>an</th>\n",
       "      <th>career</th>\n",
       "      <th>current</th>\n",
       "      <th>data</th>\n",
       "      <th>deep</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>of</th>\n",
       "      <th>science</th>\n",
       "      <th>set</th>\n",
       "      <th>sub</th>\n",
       "      <th>the</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.230768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230768</td>\n",
       "      <td>0.648673</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    amazing        an    career   current      data      deep        in  \\\n",
       "0  0.324336  0.324336  0.324336  0.324336  0.324336  0.000000  0.324336   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.324336  0.000000   \n",
       "\n",
       "         is  learning   machine        of   science       set       sub  \\\n",
       "0  0.230768  0.000000  0.000000  0.000000  0.324336  0.000000  0.000000   \n",
       "1  0.230768  0.648673  0.324336  0.324336  0.000000  0.324336  0.324336   \n",
       "\n",
       "        the     world  \n",
       "0  0.324336  0.324336  \n",
       "1  0.000000  0.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result.toarray(),columns=vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2425ea7",
   "metadata": {},
   "source": [
    "# Word Embeddings - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419f91c",
   "metadata": {},
   "source": [
    "Word embedding is a technique in natural language processing (NLP) that represents words as dense, continuous-valued vectors in a multi-dimensional space. The goal of word embeddings is to capture semantic relationships and meaning between words, allowing words with similar meanings to have similar vector representations. This technique helps address some of the limitations of traditional methods like one-hot encoding or bag-of-words.\n",
    "\n",
    "Word embeddings have become a cornerstone of modern NLP because they enable algorithms to better understand the context and meaning of words, leading to improved performance in various language-related tasks.\n",
    "\n",
    "Key characteristics of word embeddings:\n",
    "\n",
    "    *Semantic Relationships: Words with similar meanings or that are used in similar contexts have vectors that are closer together in the embedding space.\n",
    "\n",
    "    *Analogies: Word embeddings can capture analogical relationships, such as \"king - man + woman = queen,\" by performing vector arithmetic in the embedding space.\n",
    "\n",
    "    *Contextual Information: In more advanced embeddings like contextual embeddings, the meaning of a word depends on its surrounding words in a sentence, allowing the model to capture word sense disambiguation.\n",
    "\n",
    "    *Dimension Reduction: Word embeddings represent words in a lower-dimensional space compared to the large and sparse dimensions of traditional approaches, which reduces computational complexity.\n",
    "\n",
    "    *Transferable Knowledge: Pre-trained embeddings can be used as starting points for various NLP tasks, even when you have limited data.\n",
    "\n",
    "Common word embedding methods:\n",
    "\n",
    "    *Word2Vec: Introduced by Google, Word2Vec trains word vectors by predicting surrounding words in a context window. It has two architectures: \n",
    "            *Continuous Bag of Words (CBOW) \n",
    "            *Skip-gram.\n",
    "\n",
    "    *GloVe (Global Vectors for Word Representation): GloVe learns word vectors by factorizing the word co-occurrence matrix.\n",
    "\n",
    "    *FastText: FastText extends Word2Vec by representing words as subword units (character n-grams). This enables handling of out-of-vocabulary words and capturing morphological information.\n",
    "\n",
    "    *BERT (Bidirectional Encoder Representations from Transformers): BERT is a transformer-based model that generates contextualized embeddings by considering the entire context of a word in a sentence.\n",
    "\n",
    "    *ELMo (Embeddings from Language Models): ELMo generates contextualized word embeddings using deep bi-directional language models.\n",
    "\n",
    "Word embeddings have revolutionized NLP by enabling models to understand language in a more nuanced and semantically meaningful way. They are used for various tasks including sentiment analysis, named entity recognition, machine translation, question answering, text classification, and more. Pre-trained embeddings from large-scale language models can be fine-tuned on specific tasks, further enhancing their applicability and performance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6a196a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f00ce7a",
   "metadata": {},
   "source": [
    "## Skip Gram Architecture-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36167236",
   "metadata": {},
   "source": [
    "The Skip-Gram architecture is one of the two main architectures used in the Word2Vec model, which is a popular method for learning word embeddings from large text corpora. The Skip-Gram architecture is designed to predict context words (words surrounding a target word) given a target word. It's particularly effective at capturing semantic relationships between words.\n",
    "\n",
    "Here's how the Skip-Gram architecture works:\n",
    "\n",
    "    * Objective: The objective of Skip-Gram is to maximize the probability of predicting context words based on a given target word.\n",
    "\n",
    "    * Input-Output Pairs: For each target word in the input corpus, a set of context words is selected within a certain window size given around the target word. These pairs of target and context words serve as the input-output pairs for training.\n",
    "\n",
    "    * Word Representations: The target word is represented as a one-hot encoded vector. This vector is multiplied by a weight matrix (input-to-hidden weights) to obtain a dense representation called the \"input word embedding.\"\n",
    "\n",
    "    * Prediction: The input word embedding is then used to predict the probabilities of context words using a softmax activation function. Each context word has its own output weights in a shared output weight matrix.\n",
    "\n",
    "    * Loss Function: The model's objective is to minimize the negative log likelihood (cross-entropy loss) between the predicted probabilities and the actual context words' one-hot encoded vectors.\n",
    "\n",
    "    * Training: During training, the weights of both the input-to-hidden and output layers are updated using gradient descent to minimize the loss.\n",
    "\n",
    "The key idea of the Skip-Gram architecture is to learn word embeddings by adjusting the weights such that the model can accurately predict the context words given the target word. By doing so, the model learns to place similar words closer to each other in the embedding space, capturing semantic relationships.\n",
    "\n",
    "The Skip-Gram architecture has some advantages:\n",
    "\n",
    "    It's capable of capturing more fine-grained relationships between words.\n",
    "    It works well with small training datasets.\n",
    "    It's particularly useful for building embeddings on rare words.\n",
    "However, the Skip-Gram architecture can be computationally more expensive compared to the Continuous Bag of Words (CBOW) architecture, which is the other main architecture of Word2Vec. CBOW predicts the target word based on the surrounding context words. The choice between Skip-Gram and CBOW depends on the specific NLP task, the size of the dataset, and the desired quality of the word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e619c7d",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dcaf05",
   "metadata": {},
   "source": [
    "The Continuous Bag of Words (CBOW) architecture is one of the two main architectures used in the Word2Vec model, which is designed to learn word embeddings from large text corpora. The CBOW architecture aims to predict a target word based on its surrounding context words. It's particularly effective at capturing syntactic relationships between words.\n",
    "\n",
    "Here's how the CBOW architecture works:\n",
    "\n",
    "    Objective: The objective of CBOW is to predict a target word based on the context words (words surrounding the target word) within a certain window.\n",
    "\n",
    "    Input-Output Pairs: For each target word in the input corpus, a set of context words is selected within a certain window around the target word. These pairs of context words and target words serve as the input-output pairs for training.\n",
    "\n",
    "    Context Words: The context words are represented as one-hot encoded vectors. These vectors are averaged to obtain a single \"context vector,\" which represents the context of the target word.\n",
    "\n",
    "    Projection Layer: The context vector is multiplied by a projection (hidden) weight matrix to obtain a projected representation. This is essentially a dense vector that encodes the context.\n",
    "\n",
    "    Prediction: The projected context vector is used to predict the probabilities of different words in the vocabulary using a softmax activation function. The goal is to maximize the probability of the actual target word.\n",
    "\n",
    "    Loss Function: The model's objective is to minimize the negative log likelihood (cross-entropy loss) between the predicted probabilities and the actual target word's one-hot encoded vector.\n",
    "\n",
    "    Training: During training, the weights of both the projection layer and the output layer are updated using gradient descent to minimize the loss.\n",
    "\n",
    "The key idea of the CBOW architecture is to learn word embeddings by adjusting the projection weights so that the model can accurately predict the target word based on its context. By doing so, the model learns to place words with similar contexts closer to each other in the embedding space, capturing syntactic relationships.\n",
    "\n",
    "CBOW has some advantages:\n",
    "\n",
    "    It's computationally more efficient than the Skip-Gram architecture, especially for large training datasets.\n",
    "    It's more effective for frequent words and common language patterns.\n",
    "\n",
    "However, CBOW might not capture very fine-grained relationships between words as effectively as the Skip-Gram architecture does. The choice between CBOW and Skip-Gram depends on the specific NLP task, the size of the dataset, and the desired quality of the word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284434e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fda894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9490df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba8d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eae766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
