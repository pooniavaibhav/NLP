{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy-\n",
    "* Open source Natural Language Processing Library\n",
    "* For multple NLP tasks spacy has one implemented methods choosing the most efficient algorithm currently available.\n",
    "* So we can not choose the other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK-\n",
    "* NLTK is other popular open source NLP library.\n",
    "* It is old but includes less efficient algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK vs Spacy -\n",
    "Spacy does not include pre-created models for some applications like sentiment analysis, which is typically easier to perform with NLTK. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP-\n",
    "NLP is an area of computer science and AI concerned with the interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPACY Basics \n",
    "* .text = original word text.\n",
    "* ._lemma_ = The base form of the word.\n",
    "* .pos_ = The simple part-of speech tag\n",
    "* tag_ = The detailed part of speech tag\n",
    "* shape_ = The word shape - capitlization,punctuation, digits.\n",
    "* .is_alpha = Is the token an alpha character?\n",
    "* .is_stop = is the token part of stop list, i.e the most common words of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this doc object contains the processed text.\n",
    "doc = nlp('Tesla is looking at buying U.S startup $6 million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.S PROPN nsubj\n",
      "startup NOUN conj\n",
      "$ SYM quantmod\n",
      "6 NUM compound\n",
      "million NUM dobj\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_,token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7fdb6d2c7860>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fdb6ce7d3a8>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fdb6ce7d408>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spacy works on a pipeline object\n",
    "# so when we passed our text through nlp object. All these operations were performed.\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting basic names-\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization-\n",
    "* The first step in processing any text is split up all the component parts i.e. words and punctuations into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization is done based on-\n",
    "* Prefix - Characters at the begining. '$'\n",
    "* Suffix - characters at the end. km\n",
    "* Infix - characters in bw=etween. '-'\n",
    "* Exceptions - special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are aplied. U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "not PART neg\n",
      "looking VERB ROOT\n",
      "into ADP prep\n",
      "start NOUN pobj\n",
      "ups NOUN advcl\n",
      "any DET advmod\n",
      "more ADV advmod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Tesla is not looking into start ups any more.\")\n",
    "for token in doc2:\n",
    "    print(token.text,token.pos_,token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spans\n",
    "Large objects can be hard to work with a times.A span is a slice of Doc object in the form Doc[start:stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(\"Databases are a great, secure, and reliable way to store data. All major relational databases have something in common — SQL — a language to manipulate databases, tables, and data. SQL is a broad topic to cover, especially when dealing with different database vendors, such as Microsoft, IBM, or Oracle, so let’s start with SQLite — the most lightweight database system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "major relational databases have something in common — SQL — a language to manipulate databases, tables, and data. SQL is a broad topic to cover, especially when dealing with different database"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_text = doc3[15:50]\n",
    "limit_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases are a great, secure, and reliable way to store data.\n",
      "All major relational databases have something in common — SQL — a language to manipulate databases, tables, and data.\n",
      "SQL is a broad topic to cover, especially when dealing with different database vendors, such as Microsoft, IBM, or Oracle, so let’s start with SQLite — the most lightweight database system.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc3.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab\n",
    "Vocab is alist of tokens a library contains.current library is 'en_core_web_sm'.It would be having a vocabulary of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.vocab)\n",
    "#so when we loaded up en_core_web_sm that has a vocab of below number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition(NER)-\n",
    "* NER are another layer of context, when we loaded a language model in the begining recognises organizational names,location etc.\n",
    "* These are available as ENTS property of entity object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc7 = nlp(\"Apple to build a hong kong factory for $6 million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "ORG\n",
      "Companies, agencies, institutions, etc.\n",
      "\n",
      "hong kong\n",
      "GPE\n",
      "Countries, cities, states\n",
      "\n",
      "$6 million\n",
      "MONEY\n",
      "Monetary values, including unit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for entity in doc7.ents:\n",
    "    print(entity)\n",
    "    print(entity.label_)\n",
    "    print(str(spacy.explain(entity.label_)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun chunks-\n",
    "Noun chunks can be defined as noun plus the word describing that noun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc7 = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc7.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is going to build a U.K. factory $6 million.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to build a \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " factory \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Often when searching for certain keyword, it helps if the search return variations of the word.\n",
    "\n",
    "* For instance, searching for 'boat' might also return \"boats\" and \"boating\".Here \"boat would be the stem for boat,boater,boating,boats\"\n",
    "\n",
    "* Stemming chops off letter from the end untill the stem is reached.\n",
    "#### But english language has too many exceptions.\n",
    "\n",
    "So we need a more sophesticated way to reach a root word so SPACY uses LEMMITIZATION."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English stemmer or perter stemmer-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner','ran','runs','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run---->run\n",
      "runner---->runner\n",
      "ran---->ran\n",
      "runs---->run\n",
      "easily---->easili\n",
      "fairly---->fairli\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(i + '---->' + p_stemmer.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snowball Stemmer-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run---->run\n",
      "runner---->runner\n",
      "ran---->ran\n",
      "runs---->run\n",
      "easily---->easili\n",
      "fairly---->fair\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(i + '---->' + s_stemmer.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization-\n",
    "* In contrast to stemming,lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply morphological analysis to words.\n",
    "* Lemmaization looks at the surrounding text to determine a given words part of speech, it does not categorize phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I am a runner running in a race because I love to run since I ran today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t -PRON-\n",
      "am \t be\n",
      "a \t a\n",
      "runner \t runner\n",
      "running \t run\n",
      "in \t in\n",
      "a \t a\n",
      "race \t race\n",
      "because \t because\n",
      "I \t -PRON-\n",
      "love \t love\n",
      "to \t to\n",
      "run \t run\n",
      "since \t since\n",
      "I \t -PRON-\n",
      "ran \t run\n",
      "today \t today\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text, '\\t',token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t -PRON- \t 561228191312463089\n",
      "am \t be \t 10382539506755952630\n",
      "a \t a \t 11901859001352538922\n",
      "runner \t runner \t 12640964157389618806\n",
      "running \t run \t 12767647472892411841\n",
      "in \t in \t 3002984154512732771\n",
      "a \t a \t 11901859001352538922\n",
      "race \t race \t 8048469955494714898\n",
      "because \t because \t 16950148841647037698\n",
      "I \t -PRON- \t 561228191312463089\n",
      "love \t love \t 3702023516439754181\n",
      "to \t to \t 3791531372978436496\n",
      "run \t run \t 12767647472892411841\n",
      "since \t since \t 10066841407251338481\n",
      "I \t -PRON- \t 561228191312463089\n",
      "ran \t run \t 12767647472892411841\n",
      "today \t today \t 11042482332948150395\n"
     ]
    }
   ],
   "source": [
    "#You can use hash values  to see if the words are breaking to a same words\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t',token.lemma_,'\\t',token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words-\n",
    "* Words like \"a\" and \"the\" appear so frequently that they dont require tagging as thoroughly as nouns,verbs and modifiers.\n",
    "* We call them stop words and they can be filtered from the text to be processed.\n",
    "* Spacy holds a built- in list of some 326 english stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a stop word in vocab-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('btw')\n",
    "nlp.vocab['btw'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n"
     ]
    }
   ],
   "source": [
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove a stop word-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.remove('btw')\n",
    "nlp.vocab['btw'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase matching-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be considered as more powerful version of REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SolarPower\n",
    "pattern1 = [{'LOWER':'solarpower'}]\n",
    "# Solar-power\n",
    "pattern2 = [{'LOWER':'solar'},{'IS_PUNCT':True},{'LOWER':'power'}]\n",
    "# Solar power\n",
    "pattern3 = [{'LOWER':'solar'},{'LOWER':'power'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('SolarPower',None,pattern1,pattern2,pattern3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The solar power industry continues to grow a solarpower increases. Solar-power is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8656102463236116519, 1, 3),\n",
       " (8656102463236116519, 8, 9),\n",
       " (8656102463236116519, 11, 14)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches = matcher(doc)\n",
    "found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
